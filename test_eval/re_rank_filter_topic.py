import json
import time
import os
import pandas as pd
import torch
from pymilvus import MilvusClient
from sentence_transformers import SentenceTransformer, CrossEncoder
from transformers import AutoModelForCausalLM, AutoTokenizer

# ==========================================
# 1. Cáº¤U HÃŒNH
# ==========================================
EMBED_MODEL = "hiieu/halong_embedding"
RERANKER_MODEL = "itdainb/PhoRanker"
LLM_MODEL_ID = "Qwen/Qwen2.5-1.5B-Instruct" # Model phÃ¢n loáº¡i query

INPUT_FOLDER = 'inputs'
INPUT_TEST_FILE = os.path.join(INPUT_FOLDER, 'ground_truth_test.json')
OUTPUT_FOLDER = f'outputs/{EMBED_MODEL.replace("/", "_")}_{RERANKER_MODEL.replace("/", "_")}_with_topic_filter'
OUTPUT_CSV_FILE = os.path.join(OUTPUT_FOLDER, 'benchmark_details_top5.csv')

MILVUS_URI = "http://127.0.0.1:19530"
COLLECTION_NAME = "globaltech_news_labeled" # Collection má»›i cÃ³ chá»©a trÆ°á»ng topic
TOP_K_EXPORT = 5

# Danh sÃ¡ch Topic chuáº©n trong DB 
VALID_TOPICS = [
    "thoi-su", "du-lich", "the-gioi", "kinh-doanh", "khoa-hoc", 
    "giai-tri", "the-thao", "phap-luat", "giao-duc", "suc-khoe", "doi-song",
    "Other",
]

# ==========================================
# 2. LOAD MODELS
# ==========================================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ðŸš€ Device: {device.upper()}")

print("â³ Loading Embedding Model...")
embed_model = SentenceTransformer(EMBED_MODEL, device=device)

print("â³ Loading Reranker...")
reranker = CrossEncoder(RERANKER_MODEL, max_length=256, device=device)

print(f"â³ Loading LLM Classifier ({LLM_MODEL_ID})...")
tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)
llm_model = AutoModelForCausalLM.from_pretrained(
    LLM_MODEL_ID,
    torch_dtype="auto",
    device_map="auto"
)

print("â³ Connecting to Milvus...")
client = MilvusClient(uri=MILVUS_URI)
client.load_collection(COLLECTION_NAME)

os.makedirs(OUTPUT_FOLDER, exist_ok=True)

# ==========================================
# 3. HÃ€M PHÃ‚N LOáº I QUERY Báº°NG LLM
# ==========================================
def classify_query(query):
    """
    DÃ¹ng Qwen Ä‘á»ƒ Ä‘oÃ¡n topic cá»§a cÃ¢u query.
    Tráº£ vá»: TÃªn topic (str) hoáº·c None náº¿u khÃ´ng cháº¯c cháº¯n.
    """
    system_prompt = f"""Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn phÃ¢n loáº¡i chá»§ Ä‘á» tin tá»©c.
Danh sÃ¡ch chá»§ Ä‘á» há»£p lá»‡: {', '.join(VALID_TOPICS)}.
Nhiá»‡m vá»¥: Chá»‰ tráº£ vá» Ä‘Ãºng tÃªn chá»§ Ä‘á» thuá»™c danh sÃ¡ch trÃªn mÃ  cÃ¢u há»i Ä‘ang Ä‘á» cáº­p Ä‘áº¿n. KhÃ´ng giáº£i thÃ­ch thÃªm. Náº¿u khÃ´ng cháº¯c cháº¯n hoáº·c khÃ´ng thuá»™c chá»§ Ä‘á» nÃ o, hÃ£y tráº£ vá» 'Other'."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"CÃ¢u há»i: \"{query}\"\nChá»§ Ä‘á»:"}
    ]
    
    text = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    model_inputs = tokenizer([text], return_tensors="pt").to(device)
    
    # Generate
    with torch.no_grad():
        generated_ids = llm_model.generate(
            **model_inputs,
            max_new_tokens=20, # Chá»‰ cáº§n output ngáº¯n
            temperature=0.1,   # Giáº£m sÃ¡ng táº¡o Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c
            do_sample=False    # DÃ¹ng Greedy decoding Ä‘á»ƒ á»•n Ä‘á»‹nh káº¿t quáº£
        )
        
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    predicted_topic = response.strip()
    
    # Kiá»ƒm tra xem output cÃ³ náº±m trong list topic chuáº©n khÃ´ng
    if predicted_topic in VALID_TOPICS:
        return predicted_topic
    else:
        # Náº¿u LLM tráº£ vá» lung tung hoáº·c 'Other', ta sáº½ khÃ´ng filter
        return None

# ==========================================
# 4. HÃ€M SEARCH (CÃ“ FILTER)
# ==========================================
def search_and_return_top_k(query_text, k=5, candidate_limit=50):
    t0 = time.time()
    
    # BÆ¯á»šC 1: PhÃ¢n loáº¡i Query
    predicted_topic = classify_query(query_text)
    
    # BÆ¯á»šC 2: Táº¡o Expression Filter
    # Náº¿u Ä‘oÃ¡n Ä‘Æ°á»£c topic -> Chá»‰ tÃ¬m trong topic Ä‘Ã³
    # Náº¿u khÃ´ng Ä‘oÃ¡n Ä‘Æ°á»£c (None) -> TÃ¬m trÃªn toÃ n bá»™ DB
    search_filter = ""
    if predicted_topic:
        search_filter = f"topic == '{predicted_topic}'"
    
    # BÆ¯á»šC 3: Embed & Search Milvus
    query_vector = embed_model.encode([query_text])
    
    search_res = client.search(
        collection_name=COLLECTION_NAME,
        data=query_vector,
        limit=candidate_limit,
        filter=search_filter, # <--- ÃP Dá»¤NG FILTER Táº I ÄÃ‚Y
        search_params={"metric_type": "COSINE", "params": {"nprobe": 64}},
        output_fields=["title", "text", "original_id", "topic"] # Láº¥y thÃªm field topic Ä‘á»ƒ debug
    )
    
    milvus_hits = search_res[0]
    
    # Fallback: Náº¿u filter quÃ¡ cháº·t mÃ  khÃ´ng ra káº¿t quáº£ nÃ o, hÃ£y thá»­ tÃ¬m láº¡i mÃ  khÃ´ng filter
    if not milvus_hits and predicted_topic:
        # print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y trong topic '{predicted_topic}'. Äang tÃ¬m kiáº¿m toÃ n cá»¥c...")
        search_res = client.search(
            collection_name=COLLECTION_NAME,
            data=query_vector,
            limit=candidate_limit,
            # KhÃ´ng truyá»n filter
            search_params={"metric_type": "COSINE", "params": {"nprobe": 64}},
            output_fields=["title", "text", "original_id", "topic"]
        )
        milvus_hits = search_res[0]

    if not milvus_hits:
        return [], (time.time() - t0), predicted_topic

    # BÆ¯á»šC 4: Rerank
    cross_inp = [[query_text, hit['entity']['text']] for hit in milvus_hits]
    cross_scores = reranker.predict(cross_inp)
    
    for idx, hit in enumerate(milvus_hits):
        hit['cross_score'] = cross_scores[idx]
        
    reranked_hits = sorted(milvus_hits, key=lambda x: x['cross_score'], reverse=True)
    final_hits = reranked_hits[:k]
    
    duration = time.time() - t0
    return final_hits, duration, predicted_topic

# ==========================================
# 5. RUN BENCHMARK
# ==========================================
def run_benchmark():
    print(f"ðŸš€ Starting benchmark with LLM Topic Filter...")
    
    try:
        with open(INPUT_TEST_FILE, 'r', encoding='utf-8') as f:
            test_cases = json.load(f)
    except FileNotFoundError:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file {INPUT_TEST_FILE}")
        return

    all_results = []
    
    for index, test_case in enumerate(test_cases):
        query = test_case['query']
        expected_id = str(test_case['doc_id'])
        case_id = test_case.get('id', index)
        
        print(f"Processing #{case_id}...", end='\r')
        
        # Gá»i hÃ m search má»›i (nháº­n thÃªm predicted_topic Ä‘á»ƒ log)
        hits, duration, predicted_topic = search_and_return_top_k(query, k=TOP_K_EXPORT)
        
        if not hits:
            all_results.append({
                "test_id": case_id,
                "query": query,
                "predicted_topic": predicted_topic, # Log xem LLM Ä‘oÃ¡n gÃ¬
                "rank": 0,
                "process_time": round(duration, 4), 
                "retrieved_id": "NOT_FOUND",
                "is_correct": False,
                "score": 0
            })
            continue

        found_in_top_k = False
        for rank, hit in enumerate(hits):
            retrieved_id = str(hit['entity'].get('original_id', ''))
            is_match = (retrieved_id == expected_id)
            if is_match: found_in_top_k = True

            row = {
                "test_id": case_id,
                "query": query,
                "predicted_topic": predicted_topic, # Log topic dá»± Ä‘oÃ¡n
                "doc_topic": hit['entity'].get('topic', ''), # Log topic thá»±c táº¿ cá»§a bÃ i tÃ¬m Ä‘Æ°á»£c
                "expected_id": expected_id,
                "rank": rank + 1,                    
                "process_time": round(duration, 4), 
                "score": round(float(hit['cross_score']), 4),
                "is_correct": is_match,              
                "retrieved_id": retrieved_id,
                "retrieved_title": hit['entity'].get('title', ''),
                "snippet": hit['entity'].get('text', '')[:200]
            }
            all_results.append(row)

    df = pd.DataFrame(all_results)
    df.to_csv(OUTPUT_CSV_FILE, index=False, encoding='utf-8-sig')
    
    correct_queries = df.groupby('test_id')['is_correct'].any().sum()
    accuracy = (correct_queries / len(test_cases)) * 100
    
    print("\n" + "="*50)
    print(f"âœ… ÄÃ£ lÆ°u káº¿t quáº£ táº¡i: {OUTPUT_CSV_FILE}")
    print(f"ðŸŽ¯ Hit Rate @ {TOP_K_EXPORT}: {accuracy:.2f}%")
    print(f"â±  Thá»i gian TB/query: {df['process_time'].mean():.4f}s")
    print("="*50)

if __name__ == "__main__":
    run_benchmark()