{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966a6fd6",
   "metadata": {},
   "source": [
    "## Check Data Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2348a197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ƒêang ki·ªÉm tra file: e:\\My Document\\GlobalTechJSC_NLP\\inputs\\gold_standard_dataset.json\n",
      "‚úÖ ƒê√£ t√¨m th·∫•y file input.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pymilvus import MilvusClient\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "# ƒê∆∞·ªùng d·∫´n t√≠nh t·ª´ folder 'test_eval' ƒëi ra ngo√†i m·ªôt c·∫•p (..) r·ªìi v√†o 'inputs'\n",
    "INPUT_FILE_PATH = '../inputs/gold_standard_dataset.json' \n",
    "\n",
    "# C·∫•u h√¨nh Milvus\n",
    "MILVUS_URI = \"http://127.0.0.1:19530\"\n",
    "COLLECTION_NAME = \"globaltech_nlp_project\"\n",
    "\n",
    "print(f\"üìÇ ƒêang ki·ªÉm tra file: {os.path.abspath(INPUT_FILE_PATH)}\")\n",
    "\n",
    "# Ki·ªÉm tra file c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "if not os.path.exists(INPUT_FILE_PATH):\n",
    "    print(\"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file input. H√£y ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.\")\n",
    "else:\n",
    "    print(\"‚úÖ ƒê√£ t√¨m th·∫•y file input.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b553f9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä T·ªïng s·ªë c√¢u query: 100\n",
      "üîç T·ªïng s·ªë doc_id duy nh·∫•t c·∫ßn ki·ªÉm tra (Ground Truths): 357\n"
     ]
    }
   ],
   "source": [
    "def load_and_extract_ids(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        # X·ª≠ l√Ω c·∫£ tr∆∞·ªùng h·ª£p file l√† JSON list ho·∫∑c JSONL\n",
    "        try:\n",
    "            data = json.load(f) # D·∫°ng chu·∫©n [...]\n",
    "        except json.JSONDecodeError:\n",
    "            f.seek(0)\n",
    "            data = [json.loads(line) for line in f] # D·∫°ng JSONL\n",
    "\n",
    "    all_ground_truth_ids = set()\n",
    "    query_map = {} # Map ƒë·ªÉ tra c·ª©u ng∆∞·ª£c: doc_id n√†y thu·ªôc c√¢u h·ªèi n√†o\n",
    "\n",
    "    for idx, item in enumerate(data):\n",
    "        q_id = item.get('id', str(idx))\n",
    "        \n",
    "        # L·∫•y danh s√°ch ground_truths\n",
    "        gts = item.get('ground_truths', [])\n",
    "        \n",
    "        for gt in gts:\n",
    "            # X·ª≠ l√Ω linh ho·∫°t: gt c√≥ th·ªÉ l√† dict ho·∫∑c string/int\n",
    "            doc_id = str(gt['doc_id']) if isinstance(gt, dict) and 'doc_id' in gt else str(gt)\n",
    "            \n",
    "            all_ground_truth_ids.add(doc_id)\n",
    "            \n",
    "            # L∆∞u l·∫°i ƒë·ªÉ l√°t n·ªØa b√°o c√°o xem doc_id b·ªã thi·∫øu thu·ªôc c√¢u h·ªèi n√†o\n",
    "            if doc_id not in query_map:\n",
    "                query_map[doc_id] = []\n",
    "            query_map[doc_id].append(q_id)\n",
    "            \n",
    "    return list(all_ground_truth_ids), query_map, len(data)\n",
    "\n",
    "# Th·ª±c thi\n",
    "target_ids, query_map, total_queries = load_and_extract_ids(INPUT_FILE_PATH)\n",
    "print(f\"üìä T·ªïng s·ªë c√¢u query: {total_queries}\")\n",
    "print(f\"üîç T·ªïng s·ªë doc_id duy nh·∫•t c·∫ßn ki·ªÉm tra (Ground Truths): {len(target_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6487ddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ B·∫Øt ƒë·∫ßu ki·ªÉm tra 357 IDs trong Milvus...\n",
      "------------------------------\n",
      "‚úÖ T√¨m th·∫•y: 357 b√†i.\n",
      "‚ùå B·ªã thi·∫øu (Missing): 0 b√†i.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "client = MilvusClient(uri=MILVUS_URI)\n",
    "\n",
    "def check_existence_in_milvus(doc_ids, collection_name, batch_size=1000):\n",
    "    found_ids = set()\n",
    "    total = len(doc_ids)\n",
    "    \n",
    "    print(f\"üöÄ B·∫Øt ƒë·∫ßu ki·ªÉm tra {total} IDs trong Milvus...\")\n",
    "    \n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = doc_ids[i : i + batch_size]\n",
    "        \n",
    "        # Query Milvus xem nh·ªØng ID n√†y c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "        # L∆∞u √Ω: Tr∆∞·ªùng ch·ª©a ID b√†i b√°o trong Milvus c·ªßa b·∫°n l√† 'original_id' hay 'id'?\n",
    "        # D·ª±a tr√™n code c≈© c·ªßa b·∫°n l√† 'original_id'.\n",
    "        res = client.query(\n",
    "            collection_name=collection_name,\n",
    "            filter=f\"original_id in {batch}\", \n",
    "            output_fields=[\"original_id\"]\n",
    "        )\n",
    "        \n",
    "        # Thu th·∫≠p nh·ªØng ID t√¨m th·∫•y\n",
    "        for item in res:\n",
    "            found_ids.add(str(item['original_id']))\n",
    "            \n",
    "    return found_ids\n",
    "\n",
    "# Th·ª±c thi ki·ªÉm tra\n",
    "found_ids = check_existence_in_milvus(target_ids, COLLECTION_NAME)\n",
    "missing_ids = set(target_ids) - found_ids\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"‚úÖ T√¨m th·∫•y: {len(found_ids)} b√†i.\")\n",
    "print(f\"‚ùå B·ªã thi·∫øu (Missing): {len(missing_ids)} b√†i.\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1168630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ TUY·ªÜT V·ªúI! 100% d·ªØ li·ªáu Ground Truth ƒë·ªÅu t·ªìn t·∫°i trong Milvus.\n"
     ]
    }
   ],
   "source": [
    "if len(missing_ids) == 0:\n",
    "    print(\"üéâ TUY·ªÜT V·ªúI! 100% d·ªØ li·ªáu Ground Truth ƒë·ªÅu t·ªìn t·∫°i trong Milvus.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: C√°c c√¢u h·ªèi sau ƒë√¢y c√≥ ƒë√°p √°n KH√îNG T·ªíN T·∫†I trong Database:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Gom nh√≥m theo c√¢u h·ªèi\n",
    "    affected_queries = {}\n",
    "    for missing in missing_ids:\n",
    "        query_ids = query_map.get(missing, [])\n",
    "        for q_id in query_ids:\n",
    "            if q_id not in affected_queries:\n",
    "                affected_queries[q_id] = []\n",
    "            affected_queries[q_id].append(missing)\n",
    "            \n",
    "    # In ra danh s√°ch\n",
    "    count = 0\n",
    "    for q_id, ids in affected_queries.items():\n",
    "        count += 1\n",
    "        print(f\"{count}. Query ID: {q_id}\")\n",
    "        print(f\"   - Missing Docs: {ids}\")\n",
    "        if count >= 20:\n",
    "            print(\"... (v√† c√≤n nhi·ªÅu c√¢u kh√°c, ch·ªâ hi·ªÉn th·ªã 20 c√¢u ƒë·∫ßu) ...\")\n",
    "            break\n",
    "\n",
    "    # T√≠nh t·ª∑ l·ªá ph·∫ßn trƒÉm d·ªØ li·ªáu h·ªèng\n",
    "    missing_ratio = (len(missing_ids) / len(target_ids)) * 100\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìâ T·ª∑ l·ªá d·ªØ li·ªáu b·ªã thi·∫øu (Data Loss): {missing_ratio:.2f}%\")\n",
    "    print(\"üëâ Khuy·∫øn ngh·ªã: B·∫°n c·∫ßn ki·ªÉm tra l·∫°i quy tr√¨nh Insert d·ªØ li·ªáu v√†o Milvus.\")\n",
    "    print(\"   C√≥ th·ªÉ nh·ªØng b√†i b√°o n√†y b·ªã l·ªói khi crawl ho·∫∑c b·ªã l·ªói khi embedding n√™n ch∆∞a ƒë∆∞·ª£c n·∫°p v√†o.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9f364",
   "metadata": {},
   "source": [
    "## Test Qwen: Qwen3 30B A3B Instruct 2507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581ae6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cdf3c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The winner of the 2018 FIFA World Cup was **France**. They defeated Croatia 4-2 in the final, which was held on July 15, 2018, at the Luzhniki Stadium in Moscow, Russia. This victory marked France's second World Cup title, their first since 1998. Kylian Mbapp√© and Antoine Griezmann were key players for France in the tournament.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    extra_body={},\n",
    "    model=\"qwen/qwen3-30b-a3b-instruct-2507\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Who won the world cup in 2018?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc21c4b8",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af2e7d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: CUDA\n",
      "1. Loading Embedding Model...\n",
      "2. Loading Reranker Model...\n",
      "3. Connecting to Milvus...\n",
      "4. Setting up LLM Client...\n",
      "Setup Complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pymilvus import MilvusClient\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import logging as transformers_logging\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "transformers_logging.set_verbosity_error()\n",
    "\n",
    "# Load environment variables\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\") \n",
    "\n",
    "# Models Config\n",
    "EMBED_MODEL_NAME = \"hiieu/halong_embedding\"\n",
    "RERANKER_MODEL_NAME = \"itdainb/PhoRanker\"\n",
    "LLM_MODEL_NAME = \"qwen/qwen3-30b-a3b-instruct-2507\" # Model b·∫°n ch·ªçn\n",
    "\n",
    "# Milvus Config\n",
    "MILVUS_URI = \"http://127.0.0.1:19530\"\n",
    "COLLECTION_NAME = \"globaltech_nlp_project\"\n",
    "\n",
    "# Paths\n",
    "INPUT_TEST_FILE = '../inputs/gold_standard_dataset.json' # ƒê∆∞·ªùng d·∫´n file input\n",
    "OUTPUT_CSV_FILE = '../outputs/rag_final_answers.csv' # ƒê∆∞·ªùng d·∫´n file output\n",
    "\n",
    "# --- SETUP DEVICES ---\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device.upper()}\")\n",
    "\n",
    "# --- LOAD MODELS ---\n",
    "print(\"1. Loading Embedding Model...\")\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME, device=device)\n",
    "\n",
    "print(\"2. Loading Reranker Model...\")\n",
    "reranker = CrossEncoder(RERANKER_MODEL_NAME, max_length=256, device=device)\n",
    "\n",
    "print(\"3. Connecting to Milvus...\")\n",
    "milvus_client = MilvusClient(uri=MILVUS_URI)\n",
    "milvus_client.load_collection(COLLECTION_NAME)\n",
    "\n",
    "print(\"4. Setting up LLM Client...\")\n",
    "llm_client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=OPENROUTER_API_KEY\n",
    ")\n",
    "\n",
    "print(\"Setup Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c068d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rerank(query_text, top_k_candidates=20, final_top_k=5):\n",
    "    \"\"\"\n",
    "    T√¨m ki·∫øm v√† s·∫Øp x·∫øp l·∫°i c√°c t√†i li·ªáu li√™n quan nh·∫•t.\n",
    "    Tr·∫£ v·ªÅ: List c√°c ƒëo·∫°n vƒÉn b·∫£n (text) t·ªët nh·∫•t.\n",
    "    \"\"\"\n",
    "    # 1. Retrieval (Vector Search)\n",
    "    query_vector = embed_model.encode([query_text])\n",
    "    search_res = milvus_client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        data=query_vector,\n",
    "        limit=top_k_candidates,\n",
    "        search_params={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 64}},\n",
    "        output_fields=[\"title\", \"text\"]\n",
    "    )\n",
    "    hits = search_res[0]\n",
    "    if not hits: return []\n",
    "\n",
    "    # 2. Re-ranking\n",
    "    # T·∫°o c·∫∑p [Query, Doc] ƒë·ªÉ ch·∫•m ƒëi·ªÉm\n",
    "    cross_inp = [[query_text, hit['entity']['text']] for hit in hits]\n",
    "    cross_scores = reranker.predict(cross_inp)\n",
    "    \n",
    "    # G√°n ƒëi·ªÉm v√† sort\n",
    "    for idx, hit in enumerate(hits):\n",
    "        hit['cross_score'] = cross_scores[idx]\n",
    "    \n",
    "    sorted_hits = sorted(hits, key=lambda x: x['cross_score'], reverse=True)\n",
    "    \n",
    "    # L·∫•y top K vƒÉn b·∫£n t·ªët nh·∫•t ƒë·ªÉ l√†m context\n",
    "    best_docs = [hit['entity']['text'] for hit in sorted_hits[:final_top_k]]\n",
    "    return best_docs\n",
    "\n",
    "def generate_answer(query, context_docs):\n",
    "    \"\"\"\n",
    "    G·ªçi LLM ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi d·ª±a tr√™n context.\n",
    "    \"\"\"\n",
    "    if not context_docs:\n",
    "        context_str = \"Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan.\"\n",
    "    else:\n",
    "        # N·ªëi c√°c ƒëo·∫°n vƒÉn b·∫£n l·∫°i, c√°ch nhau b·∫±ng xu·ªëng d√≤ng\n",
    "        context_str = \"\\n\\n\".join([f\"Document {i+1}: {doc}\" for i, doc in enumerate(context_docs)])\n",
    "\n",
    "    # Prompt Template\n",
    "    system_prompt = \"\"\"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p trong ph·∫ßn Context b√™n d∆∞·ªõi.\n",
    "N·∫øu th√¥ng tin kh√¥ng c√≥ trong Context, h√£y n√≥i 'T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p'.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Context:\n",
    "{context_str}\n",
    "\n",
    "Question: \n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = llm_client.chat.completions.create(\n",
    "            extra_body={},\n",
    "            model=LLM_MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error generating answer: {e}\"\n",
    "\n",
    "# H√†m ch·∫°y RAG cho 1 query\n",
    "def run_rag_pipeline(query):\n",
    "    # B∆∞·ªõc 1: L·∫•y context (Top 5 docs t·ªët nh·∫•t sau khi rerank)\n",
    "    context_docs = retrieve_and_rerank(query, top_k_candidates=100, final_top_k=5)\n",
    "    \n",
    "    # B∆∞·ªõc 2: Sinh c√¢u tr·∫£ l·ªùi\n",
    "    answer = generate_answer(query, context_docs)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091dd8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading Data...\n",
      "üöÄ Starting RAG Generation for 100 queries...\n",
      "--------------------------------------------------\n",
      "[100/100] Processing: Logo t√°o tr√™n iPhone 17 Pro Max s·∫Ω ƒë∆∞·ª£c ƒë·∫∑t ·ªü ƒë√¢u?...\n",
      "‚úÖ Completed processing 100 queries.\n",
      "üíæ Results saved to: ../outputs/rag_final_answers.csv\n",
      "\n",
      "üîç Preview (First 5 rows):\n",
      "                                                                                                                    query                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         answer\n",
      "0                                            NHNN s·∫Ω l√†m g√¨ ƒë·ªÉ x·ª≠ l√Ω c√°c h√†nh vi vi ph·∫°m trong ho·∫°t ƒë·ªông kinh doanh v√†ng?  NHNN s·∫Ω tƒÉng c∆∞·ªùng ho·∫°t ƒë·ªông thanh tra, ki·ªÉm tra ƒë·ªÉ x·ª≠ l√Ω nghi√™m c√°c h√†nh vi vi ph·∫°m; tƒÉng c∆∞·ªùng ki·ªÉm tra, ki·ªÉm so√°t n·ªôi b·ªô ƒë·ªëi v·ªõi vi·ªác th·ª±c hi·ªán c√°c quy ƒë·ªãnh v·ªÅ c·∫•p t√≠n d·ª•ng v√† kinh doanh v√†ng, k·ªãp th·ªùi ph√°t hi·ªán c√°c d·∫•u hi·ªáu thao t√∫ng gi√°, li√™n k·∫øt n√¢ng gi√° t·∫°o khan hi·∫øm gi·∫£. ƒê·ªìng th·ªùi, NHNN s·∫Ω ph·ªëi h·ª£p v·ªõi B·ªô C√¥ng an, B·ªô T√†i ch√≠nh v√† c√°c ƒë·ªãa ph∆∞∆°ng ƒë·ªÉ k·ªãp th·ªùi ph√°t hi·ªán, x·ª≠ l√Ω nghi√™m minh c√°c h√†nh vi vi ph·∫°m, ngƒÉn ng·ª´a nguy c∆° g√¢y b·∫•t ·ªïn th·ªã tr∆∞·ªùng. Ngo√†i ra, NHNN s·∫Ω ban h√†nh Th√¥ng t∆∞ h∆∞·ªõng d·∫´n th·ª±c hi·ªán Ngh·ªã ƒë·ªãnh 232, c·ª• th·ªÉ h√≥a h·ªì s∆° ƒëƒÉng k√Ω c·ªßa doanh nghi·ªáp v√† ng√¢n h√†ng th∆∞∆°ng m·∫°i trong vi·ªác c·∫•p ph√©p nh·∫≠p kh·∫©u, s·∫£n xu·∫•t v√†ng mi·∫øng, v√† tri·ªÉn khai c√°c gi·∫£i ph√°p ƒëi·ªÅu h√†nh th·ªã tr∆∞·ªùng v√†ng theo c∆° ch·∫ø m·ªõi.\n",
      "1              T·ªïng th·ªëng Trump c√≥ √Ω ƒë·ªãnh √°p d·ª•ng giai ƒëo·∫°n hai ho·∫∑c giai ƒëo·∫°n ba c·ªßa c√°c bi·ªán ph√°p tr·ª´ng ph·∫°t Nga kh√¥ng?                 T·ªïng th·ªëng Trump c√≥ √Ω ƒë·ªãnh √°p d·ª•ng giai ƒëo·∫°n hai c·ªßa c√°c bi·ªán ph√°p tr·ª´ng ph·∫°t Nga.\\n\\nC√°c t√†i li·ªáu cung c·∫•p cho th·∫•y T·ªïng th·ªëng Donald Trump ƒë√£ tuy√™n b·ªë s·∫µn s√†ng chuy·ªÉn sang \"giai ƒëo·∫°n hai\" trong vi·ªác tr·ª´ng ph·∫°t Nga li√™n quan ƒë·∫øn xung ƒë·ªôt ·ªü Ukraina. C·ª• th·ªÉ, √¥ng ƒë√£ ph√°t bi·ªÉu r√µ r√†ng v·ªÅ ƒëi·ªÅu n√†y v√†o ng√†y 7.9, khi tr·∫£ l·ªùi ph·ªèng v·∫•n c·ªßa ph√≥ng vi√™n, x√°c nh·∫≠n: ‚ÄúR·ªìi, t√¥i ƒë√£ s·∫µn s√†ng‚Äù khi ƒë∆∞·ª£c h·ªèi v·ªÅ kh·∫£ nƒÉng chuy·ªÉn sang giai ƒëo·∫°n tr·ª´ng ph·∫°t th·ª© hai. Tuy nhi√™n, √¥ng kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn \"giai ƒëo·∫°n ba\" hay b·∫•t k·ª≥ k·∫ø ho·∫°ch n√†o v∆∞·ª£t qu√° giai ƒëo·∫°n hai.\\n\\nDo ƒë√≥, d·ª±a tr√™n th√¥ng tin trong c√°c t√†i li·ªáu, ch·ªâ c√≥ b·∫±ng ch·ª©ng r√µ r√†ng v·ªÅ √Ω ƒë·ªãnh √°p d·ª•ng **giai ƒëo·∫°n hai**, ch·ª© kh√¥ng c√≥ th√¥ng tin n√†o n√≥i ƒë·∫øn giai ƒëo·∫°n ba.\n",
      "2                 Vi·ªác s·ª≠ d·ª•ng c√°c lo·∫°i h√≥a ch·∫•t n√†o trong n√¥ng nghi·ªáp g√¢y ·∫£nh h∆∞·ªüng l·ªõn ƒë·∫øn s·ªë l∆∞·ª£ng c√°c lo√†i c√¥n tr√πng?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Vi·ªác s·ª≠ d·ª•ng c√°c h√≥a ch·∫•t n√¥ng nghi·ªáp g√¢y ·∫£nh h∆∞·ªüng l·ªõn ƒë·∫øn s·ªë l∆∞·ª£ng c√°c lo√†i c√¥n tr√πng.\n",
      "3                                                                     Ai b·ªã ƒë√°nh khi h·ªó tr·ª£ ng∆∞·ªùi g·∫∑p tai n·∫°n giao th√¥ng?                                                                                                                                                                                                                                                                                                                         C√°c ng∆∞·ªùi b·ªã ƒë√°nh khi h·ªó tr·ª£ ng∆∞·ªùi g·∫∑p tai n·∫°n giao th√¥ng l√†:\\n\\n- Anh Nguy·ªÖn B√° Th√†nh (32 tu·ªïi)  \\n- Anh Nguy·ªÖn B√° Trung (44 tu·ªïi)  \\n- Anh Nguy·ªÖn ƒê·ª©c Vƒ©nh (43 tu·ªïi)  \\n- Anh Nguy·ªÖn ƒê·ª©c ƒê·∫°i (37 tu·ªïi)  \\n\\nT·∫•t c·∫£ ƒë·ªÅu c√πng tr√∫ t·∫°i th√¥n V·∫°n T·ªµ, x√£ Nh√¢n Th·∫Øng, t·ªânh B·∫Øc Ninh. V·ª• vi·ªác x·∫£y ra v√†o kho·∫£ng 23 gi·ªù ng√†y 7/9/2025, khi h·ªç ƒëang h·ªó tr·ª£ hai nam thanh ni√™n b·ªã tai n·∫°n giao th√¥ng tr√™n ƒë∆∞·ªùng ƒë√™ s√¥ng ƒêu·ªëng.\n",
      "4  T·ªïng B√≠ th∆∞ T√¥ L√¢m nh·∫•n m·∫°nh ƒëi·ªÅu g√¨ v·ªÅ vai tr√≤ c·ªßa khoa h·ªçc v√† c√¥ng ngh·ªá ƒë·ªëi v·ªõi s·ª± ph√°t tri·ªÉn b·ªÅn v·ªØng c·ªßa ƒë·∫•t n∆∞·ªõc?                                                                                                                                                                                                                                                                                                                       T·ªïng B√≠ th∆∞ T√¥ L√¢m nh·∫•n m·∫°nh r·∫±ng khoa h·ªçc c√¥ng ngh·ªá, ƒë·ªïi m·ªõi s√°ng t·∫°o v√† chuy·ªÉn ƒë·ªïi s·ªë kh√¥ng ch·ªâ l√† l·ª±a ch·ªçn m√† l√† **con ƒë∆∞·ªùng s·ªëng c√≤n** ƒë·ªÉ ƒë·∫•t n∆∞·ªõc ph√°t tri·ªÉn nhanh v√† b·ªÅn v·ªØng. ƒê√¢y l√† **ƒë·ªôt ph√° chi·∫øn l∆∞·ª£c**, **ƒë·ªông l·ª±c ch·ªß y·∫øu** cho c√¥ng nghi·ªáp h√≥a, hi·ªán ƒë·∫°i h√≥a ƒë·∫•t n∆∞·ªõc trong k·ª∑ nguy√™n m·ªõi, v√† l√† **ch√¨a kh√≥a v√†ng** ƒë·ªÉ hi·ªán th·ª±c h√≥a kh√°t v·ªçng x√¢y d·ª±ng m·ªôt Vi·ªát Nam h√πng c∆∞·ªùng, th·ªãnh v∆∞·ª£ng v√†o nƒÉm 2045.\n"
     ]
    }
   ],
   "source": [
    "def load_test_data(file_path):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            # X·ª≠ l√Ω c·∫£ format JSON list [...] v√† JSONL (m·ªói d√≤ng 1 json)\n",
    "            try:\n",
    "                content = f.read()\n",
    "                data = json.loads(content)\n",
    "            except json.JSONDecodeError:\n",
    "                f.seek(0)\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        data.append(json.loads(line))\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "print(\"Loading Data...\")\n",
    "test_cases = load_test_data(INPUT_TEST_FILE)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"Starting RAG Generation for {len(test_cases)} queries...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for idx, case in enumerate(test_cases):\n",
    "    query = case.get('question') or case.get('query')\n",
    "    if not query: continue\n",
    "    \n",
    "    print(f\"[{idx+1}/{len(test_cases)}] Processing: {query[:50]}...\", end=\"\\r\")\n",
    "    \n",
    "    # Ch·∫°y pipeline\n",
    "    t0 = time.time()\n",
    "    answer = run_rag_pipeline(query)\n",
    "    latency = time.time() - t0\n",
    "    \n",
    "    results.append({\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        # \"latency\": round(latency, 2) # B·ªè comment n·∫øu mu·ªën l∆∞u th√™m th·ªùi gian ch·∫°y\n",
    "    })\n",
    "\n",
    "print(f\"\\nCompleted processing {len(results)} queries.\")\n",
    "\n",
    "# --- EXPORT TO CSV ---\n",
    "df = pd.DataFrame(results)\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV_FILE), exist_ok=True)\n",
    "df.to_csv(OUTPUT_CSV_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Results saved to: {OUTPUT_CSV_FILE}\")\n",
    "print(\"\\nPreview (First 5 rows):\")\n",
    "print(df.head().to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
